{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1093e621",
   "metadata": {},
   "source": [
    "# Fitness assay simulations\n",
    "\n",
    "In this project, I explore the limitations of inferring gene essentiality from bulk fitness assays such as transposon sequencing approaches ([TnSeq](https://www.nature.com/articles/nmeth.1377), [TraDIS](https://genome.cshlp.org/content/19/12/2308.short), [InSeq](https://www.sciencedirect.com/science/article/pii/S1931312809002819), [HITS](https://www.pnas.org/doi/abs/10.1073/pnas.0906627106)). In these papers, transposons are used to disrupt genes (somewhat randomly), and tracking frequencies of a pool (or library) of these mutants using sequencing as a readout.  Because these approaches involve competing thousands of mutants together, a mutant with a mild fitness defect may appear similar to a mutant in a gene essential for growth (say a gene like RNA Polymerase). \n",
    "\n",
    "Here, I simulate fitness assays (grounded in experimental data), and propose an approach for defining a threshold beyond which it is not possible to distinguish deleterious mutations from mutations in essential genes.\n",
    "\n",
    "### Definitions/Terminology\n",
    "\n",
    "- Essential gene: here I work with a very strict defition of essentiality. A gene is essential if a cell cannot grow without it. As an example, a 50% reduction in growth rate is not considered essentiality under this strict defition. \n",
    "- Neutral gene: a gene which when lost has no impact on cellular growth.\n",
    "- Deleterious genes: genes which when lost lead to a reduction in growth rate. Note that deleterious genes lie in a continuum between essential genes and neutral genes.\n",
    "- Fitness: the per-generation log-fold change in frequency of a mutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03383650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b6f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0204b32",
   "metadata": {},
   "source": [
    "### Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09aec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anuraglimdi/github/2022_Limdi_limits-pooled-fitness-assays/AnalysisNotebooks\n"
     ]
    }
   ],
   "source": [
    "#current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a27eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anuraglimdi/github/2022_Limdi_limits-pooled-fitness-assays\n"
     ]
    }
   ],
   "source": [
    "#use the pathlib.Path function to get the parent directories-> goal is to navigate to directory with the metadata\n",
    "# and the fitness trajectories data\n",
    "path = pathlib.Path(cwd)\n",
    "repo = str(path.parents[0])\n",
    "print(path.parents[0]) #this should be the base directory for the github repository: the exact path will differ for \n",
    "#each unique user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1cc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths for metadat and mutant trajectories\n",
    "metadata_path = repo+'/Metadata/'\n",
    "data_path = repo+'/ProcessedData/Mutant_Trajectories/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16410ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#names of libraries\n",
    "libraries = ['REL606']\n",
    "#more interpretable names for the figures in the paper\n",
    "libraries2 = ['Anc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2fe026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the pandas file with all the metadata!\n",
    "all_data = pd.read_csv(metadata_path+\"all_metadata_REL606.txt\", sep=\"\\t\")\n",
    "names = all_data.iloc[:,0]\n",
    "gene_start = all_data.iloc[:,3]\n",
    "gene_end = all_data.iloc[:,4]\n",
    "strand = all_data.iloc[:,5]\n",
    "locations = np.transpose(np.vstack([gene_start,gene_end,strand]))\n",
    "k12_tags = all_data.iloc[:,2]\n",
    "uniprot_rel606 = all_data.iloc[:,6]\n",
    "\n",
    "#genomic coordinates of pseudogenes\n",
    "locations_pseudogenes = np.loadtxt(metadata_path+'pseudogenes_locations_REL606.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf58ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fractions of the gene at the 5' and 3' ends to be excluded from analysis because they insertions there may not actually\n",
    "#be disruptive to protein function\n",
    "frac5p = 0.1\n",
    "frac3p = 0.25\n",
    "\n",
    "#reading the REL606 reference genome\n",
    "with open(metadata_path+\"rel606_reference.fasta\") as in_handle:\n",
    "    for title, seq in SimpleFastaParser(in_handle):\n",
    "        ta_sites = [m.start(0) for m in re.finditer('TA', seq)]\n",
    "ta_sites = np.array(ta_sites)\n",
    "\n",
    "#counting how many TA sites are present in the interior of each gene\n",
    "ta_gene = np.zeros(len(names))\n",
    "for i in range(0,len(names)):\n",
    "    start = locations[i, 0]\n",
    "    end = locations[i, 1]\n",
    "    length = end - start\n",
    "    #if the gene is on the forward strand\n",
    "    if locations[i,2]==1:\n",
    "        #counting sites only in the middle of the gene, excluding defined fractions at each end\n",
    "        ta_gene[i] = np.sum((ta_sites > start+length*frac5p)&(ta_sites < end - length*frac3p))\n",
    "    elif locations[i,2]==-1:\n",
    "        ta_gene[i] = np.sum((ta_sites < start+length*frac5p)&(ta_sites > end - length*frac3p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcee4d",
   "metadata": {},
   "source": [
    "### Loading the mutant trajectories data for the LTEE Ancestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fae4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the information from the fitness assay condensed into a couple of matrices\n",
    "counts_all_green = np.zeros([5, len(ta_sites)])\n",
    "counts_all_red = np.zeros([5, len(ta_sites)])\n",
    "\n",
    "#loading the new file where I have the counts for each TA site for all time points\n",
    "gname = data_path+'/green_'+libraries[0]+'_merged_all_TAsites.txt'\n",
    "greendata = np.loadtxt(gname)\n",
    "rname = data_path+'/red_'+libraries[0]+'_merged_all_TAsites.txt'\n",
    "reddata = np.loadtxt(rname)\n",
    "#now for extracting the UMI corrected counts\n",
    "counts_all_green = greendata[2:11:2,:]\n",
    "counts_all_red = reddata[2:11:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7573959",
   "metadata": {},
   "source": [
    "The data is organized as follows:\n",
    "- each column corresponds to a TA transposon insertion site in the genome\n",
    "- each row corresponds to a timepoint in the fitness assay (we have a total of 5 timepoints)\n",
    "- green and red refer to experimental replicates of the fitness assay (they were named as such because I used red and green sharpies to label tubes in the lab)\n",
    "- the $t_{0}$ timepoint is the same for the two replicates: I took the transposon library from the freezer, extracted DNA from it, and split it into two replicate fitness assays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea894448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\t [15071951. 10926749. 15528916. 18645080. 16380071.]\n",
      "green\t [15071951. 16444999. 12387024. 14560311. 14105362.]\n"
     ]
    }
   ],
   "source": [
    "#number of sequencing reads in the \"red\" replicate\n",
    "print('red\\t', counts_all_red.sum(axis=1))\n",
    "#number of sequencing reads in the \"red\" replicate\n",
    "print('green\\t', counts_all_green.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15297753",
   "metadata": {},
   "source": [
    "## Mutant trajectories at different sequencing depths\n",
    "\n",
    "Approach:\n",
    "\n",
    "- Downsample the fitness assay counts by the same factor across timepoints\n",
    "- Compare mutant trajectories for the same genes (spanning a range of fitnesses) at different sequencing depths\n",
    "- Compare the estimated fitness from downsampled data to the \"true\" fitness (from the highest sequencing depth), and observe how this changes with sequencing depth.\n",
    "\n",
    "Questions we're exploring:\n",
    "\n",
    "- Do mutant trajectories for deleterious and essential genes start becoming more similar as sequencing depth decreases? This would start to hint at a key point of the paper: our ability to distinguish deleterious fitness effects gets worse with less sequencing depth\n",
    "- Is there a point beyond which increasing sequencing depth does not lead to a meaningful improvement in convergence to \"true\" fitness (while acknowledging that we don't have a ground truth)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11d6abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data, scale):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    - data: counts matrix for bulk fitness assay\n",
    "    - scale: scaling factor for downsampling, must be greater than 1.\n",
    "    \n",
    "    Process:\n",
    "    - downsample number of reads mapping to an insertion site as follows (for each time point)\n",
    "    - use np.repeat to get an list with every insertion site repeated N times, where N is the number of mapped reads\n",
    "    - use np.shuffle to rearrange this list\n",
    "    - pick the first 1/scale fraction of this list\n",
    "    - use np.unique to which sites are represented, and how frequently after downsampling.\n",
    "    - return this downsampled array\n",
    "    \n",
    "    Output:\n",
    "    - data_scaled: same shape as data but each row of the matrix downsampled by the scaling factor\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23933d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
